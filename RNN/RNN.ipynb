{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "\n",
    "# .load_data_time_machine() is implemented in language-model.ipynb\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use one_hot encoding\n",
    "# Assume we have len(vocab) = 3,\n",
    "# then 0 = [1, 0, 0], 1 = [0, 1, 0], 2 = [0, 0, 1]\n",
    "F.one_hot(torch.tensor([0, 2]), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to initialize parameters of RNN\n",
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - vocab_size: length of our input vocabulary list. eg: [\"I\", \"like\", \"fruit\"] has a length of 3\n",
    "    - num_hiddens: number of hidden neurons\n",
    "    - device: CPU or GPU\n",
    "\n",
    "    Returns:\n",
    "    [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    \"\"\"\n",
    "    num_inputs = num_outputs = vocab_size \n",
    "\n",
    "    # This is a nested function for parameter initialization \n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "    \n",
    "    W_xh = normal((num_inputs, num_hiddens))  # Weight vector from input layer to hidden layer\n",
    "    W_hh = normal((num_hiddens, num_hiddens)) # W V from previous hidden layer to current hidden layer\n",
    "    b_h = torch.zero(num_hiddens, device=device) # bias vector -- y_hidden = W_xh @ X_input + W_hh @ previous_hidden + b_h\n",
    "    W_hq = normal((num_hiddens, num_outputs)) # W V from hidden layer to output layer\n",
    "    b_q = torch.zeros(num_outputs, device=device) # bias vector used for the output\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "\n",
    "    # Set .requires_grad_(True) for training\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model function RNN\n",
    "def rnn(inputs, state, params):\n",
    "    \"\"\"\n",
    "    Implements a simple Recurrent Neural Network (RNN) forward pass.\n",
    "\n",
    "    Args:\n",
    "    - inputs: List of input tensors, each with shape (n, h), where:\n",
    "        n = batch size (number of samples processed in parallel)\n",
    "        h = input feature dimension\n",
    "    - state: Tuple containing the initial hidden state, (H,), where:\n",
    "        H has shape (n, h) and represents the hidden state of the RNN\n",
    "    - params: Tuple containing model parameters:\n",
    "        W_xh: Weight matrix for input-to-hidden connection (shape: h x h)\n",
    "        W_hh: Weight matrix for hidden-to-hidden transition (shape: h x h)\n",
    "        b_h: Bias vector for hidden state computation (shape: 1 x h)\n",
    "        W_hq: Weight matrix for hidden-to-output transformation (shape: h x q)\n",
    "        b_q: Bias vector for output computation (shape: 1 x q)\n",
    "\n",
    "    Returns:\n",
    "    - outputs: Tensor of shape (a * n, q), where:\n",
    "        a = number of time steps (length of inputs list)\n",
    "        n = batch size\n",
    "        q = output dimension\n",
    "    - final_state: Tuple containing the final hidden state (H,), where H has shape (n, h)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack model parameters\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "\n",
    "    H, = state  # H has shape (n, h)\n",
    "    # Example inputs:\n",
    "    # inputs: a list of 3 tensors (time steps), each of shape (n=2, h=5)\n",
    "    # state: (torch.zeros(n=2, h=10)) -> shape (2,10)\n",
    "    # Example parameter shapes:\n",
    "    # W_xh: (5,10), W_hh: (10,10), b_h: (1,10)\n",
    "    # W_hq: (10,5), b_q: (1,5)\n",
    "\n",
    "    # Example input sequence:\n",
    "    # Suppose vocab_size = 5\n",
    "    # inputs = [\n",
    "    #    torch.tensor([[1, 0, 0, 0, 0], [0, 1, 0, 0, 0]]),  # Time step 1\n",
    "    #    torch.tensor([[0, 1, 0, 0, 0], [0, 0, 1, 0, 0]]),  # Time step 2\n",
    "    #    torch.tensor([[0, 0, 1, 0, 0], [0, 0, 0, 1, 0]])   # Time step 3\n",
    "    # ]  # Shape: (3 time steps, 2 batches, 5 input features)\n",
    "\n",
    "    # List to store output tensors at each time step\n",
    "    outputs = []\n",
    "\n",
    "    # Iterate over the input sequence \n",
    "    for X in inputs:\n",
    "        \"\"\"\n",
    "        Compute the new hidden state:\n",
    "        H = tanh(X @ W_xh + H @ W_hh + b_h)\n",
    "        \n",
    "        Explanation:\n",
    "        - X @ W_xh: Projects input X into the hidden space (shape: n x h)\n",
    "        - H @ W_hh: Applies transformation to the previous hidden state (shape: n x h)\n",
    "        - b_h: Bias term added to the hidden state (shape: 1 x h, broadcasted to n x h)\n",
    "        - tanh: Non-linear activation function to introduce non-linearity\n",
    "        \"\"\"\n",
    "\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)  # H shape: (n, h)\n",
    "\n",
    "        \"\"\"\n",
    "        Compute the output Y at the current time step:\n",
    "        Y = H @ W_hq + b_q\n",
    "        \n",
    "        Explanation:\n",
    "        - H @ W_hq: Maps hidden state to output space (shape: n x q)\n",
    "        - b_q: Bias term for output transformation (shape: 1 x q, broadcasted to n x q)\n",
    "        \"\"\"\n",
    "        Y = torch.mm(H, W_hq) + b_q  # Y shape: (n, q)\n",
    "\n",
    "        # Store the output for this time step\n",
    "        outputs.append(Y)  # List of shape (a elements, each of shape n x q)\n",
    "\n",
    "    \"\"\"\n",
    "    Concatenate all output tensors along the first dimension:\n",
    "    - Since outputs is a list of a tensors, each of shape (n, q), we stack them\n",
    "    - The resulting tensor has shape (a * n, q), effectively flattening time dimension\n",
    "    \"\"\"\n",
    "    return torch.cat(outputs, dim=0), (H,)  # Final output shape: (a * n, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel:\n",
    "    def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn):\n",
    "        \"\"\"\n",
    "        Initializes the RNN model.\n",
    "\n",
    "        Args:\n",
    "        - vocab_size (int): Size of the vocabulary (number of unique tokens).\n",
    "        - num_hiddens (int): Number of hidden units in the RNN.\n",
    "        - device (torch.device): The device (CPU/GPU) where computations will be performed.\n",
    "        - get_params (function): Function that initializes model parameters (weights & biases).\n",
    "        - init_state (function): Function to initialize the hidden state.\n",
    "        - forward_fn (function): Function implementing the forward pass of the RNN.\n",
    "\n",
    "        Example:\n",
    "        >>> model = RNNModel(vocab_size=5, num_hiddens=10, device=torch.device('cpu'), \n",
    "        ...                  get_params=get_params, init_state=init_state, forward_fn=rnn)\n",
    "\n",
    "        This initializes:\n",
    "        - `params`: Weight matrices and biases used in the RNN.\n",
    "        - `init_state`: Function for initializing the hidden state.\n",
    "        - `forward_fn`: Function that performs the RNN computation.\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens  \n",
    "\n",
    "        # Initialize parameters (W_xh, W_hh, b_h, W_hq, b_q)\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "\n",
    "        # Store hidden state initialization and forward pass function\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "    \n",
    "    # This function is called when RNNModel gets instantiated, eg. RNN = RNNModel(...)\n",
    "    def __call__(self, X, state):\n",
    "        \"\"\"\n",
    "        Runs the forward pass of the RNN.\n",
    "\n",
    "        Args:\n",
    "        - X (torch.Tensor): Tensor of shape (batch_size, sequence_length), \n",
    "                            containing token indices for each batch.\n",
    "        - state (tuple): Tuple containing the hidden state(s).\n",
    "\n",
    "        Returns:\n",
    "        - output (torch.Tensor): Tensor of shape (sequence_length * batch_size, vocab_size),\n",
    "                                 representing the predicted output for each input token.\n",
    "        - updated state (tuple): The hidden state after processing the input.\n",
    "\n",
    "        Example:\n",
    "        X = torch.tensor([[0, 1, 2], [3, 4, 0]])  # Shape: (batch_size=2, sequence_length=3)\n",
    "        state = model.begin_state(batch_size=2, device=torch.device('cpu'))\n",
    "        output, new_state = model(X, state)\n",
    "\n",
    "        Processing:\n",
    "        One-hot Encoding: Convert token indices into one-hot vectors.\n",
    "           - Input X shape: (batch_size=2, sequence_length=3)\n",
    "           - Output shape after one-hot encoding: (sequence_length=3, batch_size=2, vocab_size=5)\n",
    "        Forward Pass: Calls forward_fn(X, state, params), which runs the RNN computation.\n",
    "        Outputs and New State: Returns the predicted outputs and updated hidden state.\n",
    "\n",
    "        Example:\n",
    "        vocab = [\"hello\", \"world\", \"I\", \"love\", \"AI\"]\n",
    "        word_to_idx = {\"hello\": 0, \"world\": 1, \"I\": 2, \"love\": 3, \"AI\": 4}\n",
    "        X = torch.tensor([[2, 3, 4],  # \"I love AI\"\n",
    "                         [0, 1, 2]]) # \"hello world I\"\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "        # Example input:\n",
    "        # Suppose vocab_size = 5, and X = [[2, 3, 4], [0, 1, 2]]\n",
    "        # The token indices map to:\n",
    "        # \"I\" → 2, \"love\" → 3, \"AI\" → 4, \"hello\" → 0, \"world\" → 1\n",
    "        #\n",
    "        # One-hot encoding transforms indices into binary vectors:\n",
    "        # \"I\"    → [0, 0, 1, 0, 0]\n",
    "        # \"love\" → [0, 0, 0, 1, 0]\n",
    "        # \"AI\"   → [0, 0, 0, 0, 1]\n",
    "        # \"hello\"→ [1, 0, 0, 0, 0]\n",
    "        # \"world\"→ [0, 1, 0, 0, 0]\n",
    "        #\n",
    "        # So the transformed X (after one-hot) has shape:\n",
    "        # (sequence_length=3, batch_size=2, vocab_size=5)\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)  \n",
    "        # Example transformation:\n",
    "        # If vocab_size = 5, and X = [[0,1,2], [3,4,0]]\n",
    "        # One-hot encoded X has shape (sequence_length=3, batch_size=2, vocab_size=5)\n",
    "\n",
    "        # Perform forward pass using the RNN function\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "    \n",
    "    def begin_state(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Initializes the hidden state for the RNN.\n",
    "        Returns:\n",
    "        - Initial hidden state of shape (batch_size, num_hiddens).\n",
    "\n",
    "        Example:\n",
    "        state = model.begin_state(batch_size=2, device=torch.device('cpu'))\n",
    "        print(state[0].shape)  # Expected: (2, num_hiddens=10)\n",
    "        \"\"\"\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2lEnvLimu3-9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
